class Zephyr7BgptqFineTuningConfig:
    MODEL_ID = "TheBloke/zephyr-7B-beta-GPTQ"
    BITS = 4
    DISABLE_EXLLAMA = True
    DEVICE_MAP = "auto"
    USE_CACHE = False
    LORA_R = 16
    LORA_ALPHA = 32
    LORA_DROPOUT = 0.05
    BIAS = "none"
    TARGET_MODULES = ["q_proj", "v_proj"]
    TASK_TYPE = "CAUSAL_LM"
    BATCH_SIZE = 1
    GRAD_ACCUMULATION_STEPS = 4
    OPTIMIZER = "paged_adamw_32bit"
    LR = 2e-4
    LR_SCHEDULER = "cosine"
    LOGGING_STEPS = 50
    SAVE_STRATEGY = "epoch"
    NUM_TRAIN_EPOCHS = 1
    MAX_STEPS = None
    DO_EVAL = True
    EVALUATION_STRATEGY = "steps"
    EVAL_ACCUMULATION_STEPS = 5
    EVAL_STEPS = 50
    MAX_EVAL_SAMPLES = 50
    LOSS_ONLY = True
    FP16 = True
    PUSH_TO_HUB = False
    DATASET_TEXT_FIELD = "" #Fill with appropiate dataset column name
    MAX_SEQ_LENGTH = 4000
    PACKING = False